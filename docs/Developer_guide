############################# Wi4MPI License ###########################
# `04/04/2016`                                                         #
# Copyright or (C) or Copr. Commissariat a l'Energie Atomique          #
#                                                                      #
# IDDN.FR.001.210022.000.S.P.2014.000.10800                            #
# This file is part of the Wi4MPI library.                             #
#                                                                      #
# This software is governed by the CeCILL-C license under French law   #
# and abiding by the rules of distribution of free software. You can   #
# use, modify and/ or redistribute the software under the terms of     #
# the CeCILL-C license as circulated by CEA, CNRS and INRIA at the     #
# following URL http://www.cecill.info.                                #
#                                                                      #
# The fact that you are presently reading this means that you have     #
# had knowledge of the CeCILL-C license and that you accept its        #
# terms.                                                               #
#                                                                      #
# Authors:                                                             #
#   - Delforge Tony <tony.delforge.tgcc@cea.fr>                        #
#   - Ducrot Vincent <vincent.ducrot.tgcc@cea.fr>                      #
#                                                                      #
########################################################################


:Authors:
    Tony Delforge (tony.delforge.tgcc@cea.fr)
    Vincent Ducrot (vincent.ducrot.tgcc@cea.fr)

Developer Guide
===============

Sommaire:
---------
1. Introduction
2. Library
    1. How it works
    2. Implémentation
        1. Library settings
        2. Symbol overload
        3. Code chooser ASM
        4. A_MPI_Function
        5. R_MPI_Function
        6. Thread safety
        7. The interface
    3. Environnement d'exécution
    4. Les Fichiers
    5. Évolution

------------

INTRODUCTION
============
MPI is a standard in HPC community which allows a simple use of clusters. Nowoaday, there are several implementation (OpenMPI, BullxMPI, MPT, IntelMPI, MPC, ...) each of which involves a specific ABI (Application Binary Interface) for an application compiled with a specific MPI implementation.
With wi4mpi, an application compiled with an alpha MPI implementation can be run under a beta MPI implementation without any compilation protocol and any concern about the ABI (Preload version).
WI4MPI can also be seen as a dedicated MPI implementation; This time, the application is compiled against the wi4mpi library (libmpi.so) whith the dedicated wrapper (mpicc,mpif90...) meant for that purpose, and can be run under any other MPI implementation (Interface Version).

Library
=======

How it works
------------

Before performing any translation we need to distinguish the application side from the runtime side. To do that, any MPI object from the application side are prefixed by A_ and those from the application are prefixed by R_.
To perform a translation, all original MPI call from the application is intercepted by WI4MPI and remplaced by the same call prefixed by A_. 
For example, with an OpenMPI ---> IntelMPI conversion:


Application: MPI_Init (OpenMPI)													MPI_Init (IntelMPI)
															\         						   /
							Phase 1					 \        						  /    Phase 3
																\       						 /
																|--------------------|
																| WI4MPI: A_MPI_Init |
																|--------------------|
																					 |
																					 |	Phase 2 
																					 | 
																			Translation		

Implémentation
--------------

Library settings
~~~~~~~~~~~~~~~~

The library is set during its loading time, when the program start. All runtime MPI routines are saved in function pointer via dlsym call to make phase 3 possible, all the table are created and set with MPI constant object, and the spinlocks are initialized. To do so, we used the following syntax:

void __attribute__((constructor)) wrapper_init
{

	void (*)lib_handle=dlopen(getenv("TRUE_MPI_LIB"),RTLD_NOW); 
	LOCAL_MPI_Function=dlsym(lib_handle,"PMPI_Function") 
	....
}

The library contain three constructor:

- wrapper_init dans test_generation_wrapper.c (API C) (preload and interface)
- wrapper_init_f dans wrapper.c (API Fortran) (preload and interface)
- wrapper_init_c2ff2c dans c2f_f2c.c (API c2f/f2c) 


Symbol overload
~~~~~~~~~~~~~~~

The mpi called are intercept thanks to the following rerouting:

- #define A_MPI_Send PMPI_Send
- #pragma weak MPI_Send=PMPI_Send

(See interface_test.c in src/interface/gen/interface_test.c and src/interface/gen/interface_fort.c)
This syntax is also present but hidden in test_generation_wrapper.c (src/{preload,interface}/gen/) whithin an asm code chooser for the next reason.

The MPI-IO implémentation (ROMIO) present within the most MPI implementation trigger some call to the MPI user interface that WI4MPI intercept thanks to its symbols overload protocol. It means that during the runtime (phase 3 above), some MPI calls are made, forcing WI4MPI to intercept them and so making the application to crash. The crash is die to the fact that WI4MPI MPI is trying to convert runtime argument to runtime version.

Example:

Application:MPI_File_open (OpenMPI)											MPI_File_open(IntelMPI)
															\         						   /											\
							Phase 1					 \        						  /    Phase 3						 \
																\       						 /													\
																|-------------------------|											|----------------------------------------------------|
																| WI4MPI: A_MPI_File_open |											| WI4MPI: A_MPI_Allreduce but with runtime arguments |
																|-------------------------|											| instead of application arguments (R_ instead of A_)|
																					 |																		|----------------------------------------------------|
																					 |	Phase 2 																										|
																					 | 																															|
																			Translation																											Translation ----> Crash




To overcome this issue, we used an ASM code chooser. 

Code chooser ASM
~~~~~~~~~~~~~~~~

The ASM code chooser does the simpl following things:

If we alreday are in the wrapper:

- The arguments are passed without any translation protocol to the underlying MPI runtime call (LOCAL_MPI_function)

Otherwise:

- The arguments are translated and passed to the underlying MPI runtime call (LOCAL_MPI_function)

To know which state the process is, we defined the in_w variable:

- in_w=1 : in the wrapper
- in_w=0 : in the application

Since the implementation of MPI objects are developer dependent, some of these may have different size among the different one. To make sure that there is no side effect, the code chooser analyze the stack itself.

ASM Code chooser implementation (generated for each function):

- .global PMPI_Function 
- .weak MPI_Function 
- .set MPI_function,PMPI_Function
- .extern in_w 
- .extern A_MPI_Function
- .extern R_MPI_Function
- .type PMPI_Function,@function
- .text
- PMPI_Function:
- push %rbp 
- mov %rsp, %rbp 
- sub $0x20, %rsp 
- mov %rdi, -0x8(%rbp) 
- mov %rsi, -0x10(%rbp) 
- mov %rdx, -0x18(%rbp) 
- mov %rcx, -0x20(%rbp) 
- .byte 0x66 
- leaq in_w@tlsgd(%rip), %rdi 
- .value 0x6666 
- rex64 
- call __tls_get_addr@PLT 
- mov -0x8(%rbp), %rdi 
- mov -0x10(%rbp), %rsi 
- mov -0x18(%rbp), %rdx 
- mov -0x20(%rbp), %rcx 
- leave 
- cmpl $0x0, 0x0(%rax) 
- jne inwrap_MPI_Function
- jmp (*)A_MPI_Function@GOTPCREL(%rip) 
- inwrap_MPI_Function:
- jmp (*)R_MPI_Function@GOTPCREL(%rip) 
- .size PMPI_Function,.-PMPI_Function

Application:MPI_File_open (OpenMPI)											MPI_File_open(IntelMPI)
															\         						   /											\
							Phase 1					 \        						  /    Phase 3						 \
																\       						 /													\
																|-------------------------|											|-------------------------|
																| WI4MPI: PMPI_File_open  |											| WI4MPI: PMPI_Allreduce  |
																| Testing in_w: in_w=0    |											| Testing in_w: in_w=1    |
																|-------------------------|											| ------------------------|
																					 |	Phase 2 																			|
																					 | 																								|
																A_MPI_File_open:Translation											R_MPI_Allreduce:No Translation

A_MPI_Function
~~~~~~~~~~~~~~
La conversion se fait à l'aide de mappers présent dans mappers.h. La plupart d'entre eux s'appuient sur plusieurs tables de hashages présentes dans new_utils.* et new_utils_fn.*.
Il y a une table pour gérer la correspondance des constantes, et une table par type MPI pour assurer la traduction.
Les différents type étant:

- MPI_Comm
- MPI_Datatype
- MPI_Errhandler
- MPI_Group
- MPI_Op
- MPI_Request **Séparé en 2 tables, afin de dissocier les requêtes persistantes des requêtes non-bloquantes**
- MPI_File

Les tables présentent dans new_utils_fn.* concernent les traductions des fonctions suivantes:

- MPI_Handler_function
- MPI_Comm_copy_attr_function
- MPI_Comm_delete_function
- MPI_Type_delete_function
- MPI_Comm_errhandler_function
- MPI_File_errhandler_function 

Exemple:

A_MPI_Send(void * buf,int count,A_MPI_Datatype datatype,int dest,int tag,A_MPI_Comm comm)

{

	void * buf_tmp;

	const_buffer_conv_a2r(&buf,&buf_tmp); **mapper**

	R_MPI_Datatype datatype_tmp;

	datatype_conv_a2r(&datatype,&datatype_tmp); **mapper**

	int dest_tmp;

	dest_conv_a2r(&dest,&dest_tmp); **mapper**

	int tag_tmp;

	tag_conv_a2r(&tag,&tag_tmp); **mapper**

	R_MPI_Comm comm_tmp;

	comm_conv_a2r(&comm,&comm_tmp); **mapper**

	int ret_tmp= LOCAL_MPI_Send( buf_tmp, count, datatype_tmp, dest_tmp, tag_tmp, comm_tmp); **Appel à MPI_Send coté runtime**

	return error_code_conv_r2a(ret_tmp); **conversion du retour d'erreur**

}


R_MPI_Function
~~~~~~~~~~~~~~
Rien de spécial ici, les arguments sont directement passés à l'appel de la fonction MPI coté runtime.

int R_MPI_Send(void * buf,int count,R_MPI_Datatype datatype,int dest,int tag,R_MPI_Comm comm)
{

	int ret_tmp= LOCAL_MPI_Send( buf, count, datatype, dest, tag, comm);

	return ret_tmp;

}

Thread safety
~~~~~~~~~~~~~
Afin de rendre le code thread safe, la variable in_w (permettant de detecter si l'on vient du wrapper ou de l'application) est protégée par une TLS:

- __thread int in_w=0; (test_wrapper_generation.c:118)
- extern __thread int in_w; (wrapper.c:7)
- extern __thread int in_w; (c2f_f2c.c:6) || (c2f_f2c.c:1149)

Pour protéger les tables, les spinlock ont été utilisés (cf :thread_safety.h):

- #define lock_dest(a) pthread_spin_destroy(a)
- #define lock_init(a) pthread_spin_init(a,PTHREAD_PROCESS_PRIVATE)
- #define lock(a)  pthread_spin_lock(a)
- #define unlock(a) pthread_spin_unlock(a)
- typedef  pthread_spinlock_t (*)table_lock_t;

L'interface:
------------
Les sections précédentes sont valables pour l'interface. Comme le montre le schéma ci-dessous l'interface rajoute un niveau de plus. Le but de cette version est de compiler une application MPI avec cette interface. Pour se faire l'interface doit ainsi exporté les symboles MPI de base.

               
              dlopen|----------|  dlopen       |---------|
                   /| Lib_OMPI | ----------- > | OpenMPI |
                  / |----------|               |---------|
   |-----------| / 
   |           |/
   | INTERFACE |
   |           |\
   |-----------| \
                  \
                   \|----------|  dlopen       |----------|
                    | Lib_IMPI | ----------- > | IntelMPI |
              dlopen|----------|               |----------|


L'interface est compilé avec les fichiers interface_fort.c et interface_test.c. Ces deux fichiers appliquent respectivement la surchage des symboles vu dans la section 2.2.2 pour l'API fortran et l'API C. L'interface effectue ensuite un dlopen vers la Lib_OMPI ou la Lib_IMPI selon la conversion choisie, représentée ci-dessous par la variable d'environnement WRAPPER_WI4MPI, afin de pouvoir appeler le code chooser ASM vu dans la section 2.2.3 (cf: dlsym en-dessous). 

int MPI_Init(int * argc,char *** argv);
#define MPI_Init PMPI_Init
#pragma weak MPI_Init=PMPI_Init
int (*INTERFACE_LOCAL_MPI_Init)(int *,char ***);

int PMPI_Init(int * argc,char *** argv)
{
#ifdef DEBUG
printf("entre : PMPI_Init (interface) \n");
#endif
int ret_tmp= INTERFACE_LOCAL_MPI_Init( argc, argv);
#ifdef DEBUG
printf("sort : PMPI_Init (interface)\n");
#endif
return ret_tmp;
}
__attribute__((constructor)) void wrapper_interface(void) {
void *interface_handle=dlopen(getenv("WRAPPER_WI4MPI"),RTLD_NOW|RTLD_GLOBAL);
if(!interface_handle)
{
    printf("no true IC lib defined\nerror :%s\n",dlerror());
    exit(1);
}
INTERFACE_LOCAL_MPI_Init=dlsym(interface_handle,"CCMPI_MPI_Init");
}



Environnement d'exécution (Lire la doc utilisateur avant de commencer cette section)
------------------------------------------------------------------------------------

WI4MPI configure son environnement seulement pour l'exécution via le script wi4mpi.

- Il set la variable d'environnement LD_PRELOAD avec le preload de l'utilisateur plus le chemin vers la bibliothèque correspondant à la conversion désirée.

- Il set la variable TRUE_MPI_LIB au chemin de la souche MPI cible

- Il export la variable LD_LIBRARY_PATH en ajoutant le chemin vers les bibliothèques vides nécessaire au bon déroulement du code.

Le problème avec ce protocole, est la perte potentiel de certains symboles MPI. Pour résoudre ce problème, la bibliothèque WI4MPI exporte ces symboles. (cf test_generation_wrapper.c).

exemple :

#if defined(OMPI_INTEL)

char ompi_mpi_comm_null[1024];

...

#endif



Les fichiers
------------

- mappers.h : contient les mappers pour la traduction
- new_utils.* et new_utils_fn.*: contient les tables
- test_wrapper_generation.c : code généré contenant le code chooser ASM, A_MPI_Function, R_MPI_Function, et l'initialisation de la bibliothèque (void __attribute__((constructor)) wrapper_init).
- wrapper.c : code généré contenant l'API Fortran , ainsi que l'initialisation des pointeurs ( void __attribute__((constructor)) wrapper_init_f)
- c2f_f2c.c : code généré contenant l'API c2f/f2c , ainsi que l'initialisation des pointeurs ( void __attribute__((constructor)) wrapper_init_c2ff2c)
- thread_safety.h : contient la définition des spin_lock
- optimisation.h : gère l'utilisation des tables
- app_mpi.h/run_mpi.h : Contiennent les constantes MPI.
- app/$FROM_$TO/app_mpi.h : contient les constantes C côté applicatif 
- app/$FROM_$TO/run_mpi.h : contient les constantes C côté runtime
- $FROM_$TO/wrapper_f.h : contient les constantes Fortran
- manual_wrapper.h : contient les fonctions/mappers écrit(es) à la main.

Évolution
---------

- Étendre la couverture MPI de WI4MPI:
  
  - Toutes les fonctions actuellement présentes dans la norme MPI 3.1 sont référencées dans functions.json. De ce fait, pour étendre la couverture de la bibliothèque WI4MPI il suffit de rajouter le nom des fonctions manquantes dans func_list.txt (cf chapitre générateur), puis de regénérer test_generation_wrapper.c avec la commande : python generator.py
  - Vis a vis des mappers, il faut rajouter ceux ne couvrant pas les types exclusivement présent dans les normes > 1.3 . C'est à dire rajouter les r2a/a2r qui vont bien dans mappers.h, et editer mappers.json en fonction des noms définis.

- Pour étendre l'utilisation de WI4MPI à plusieurs souche MPI:

  - Actuellement, le code de WI4MPI est compatible avec les conversions suivantes:
     - OMPI_OMPI : **Header généré**
     - INTEL_INTEL : **Header généré**
     - MPC_MPC : **Header non généré**
     - HPMPI_HPMPI : **Header non généré**
     - INTEL_OMPI : **Header généré**
     - OMPI_INTEL : **Header généré**
     - INTEL_MPC : **Header non généré**
     - MPC_INTEL : **Header non généré**
     - OMPI_MPC : **Header non généré**
     - MPC_OMPI : **Header non généré**
     - INTEL_HPMPI : **Header non généré**
     - HPMPI_INTEL : **Header non généré**
     - OMPI_HPMPI : **Header non généré**
     - HPMPI_OMPI : **Header non généré**
     - MPC_HPMPI : **Header non généré**
     - HPMPI_MPC : **Header non généré**
  - Générer les headers (.h) associés aux différentes conversions:
     - app_mpi.h : sed 's/MPI/A_MPI/g' mpi.h | grep sed 's/PA_MPI/A_PMPI/g'
        - cette commande effectue le plus gros du travail, pour le reste, il faut se laisser guider par le compilateur
     - run_mpi.h : sed 's/MPI/R_MPI/g' mpi.h | grep sed 's/PR_MPI/R_PMPI/g'
     - Pour les conversions de type A vers A (OMPI_OMPI, MPC_MPC, ...) attention aux redéfinitions de macros lors de la génération des headers.
  - Dans le cas ou l'on désire rajouter de nouvelles conversions:
     - Dans mappers.h, rajouter aux fonctions suivantes la compatibilité associée aux différentes conversions:
        - status_prt_conv_a2r
        - status_prt_conv_r2a
        - status_tab_conv_r2a


